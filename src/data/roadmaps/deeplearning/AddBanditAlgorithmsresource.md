# Add Bandit Algorithms resource

## Introduction

Multi-armed bandit (MAB) algorithms are used in decision-making problems where there are multiple actions with uncertain outcomes.Additive MAB algorithms are a type of MAB algorithm that assume the rewards for each action are drawn independently from a fixed distribution.These algorithms are called "additive" because they add a bonus term to the expected reward estimate, encouraging exploration of less explored actions.There are several popular additive MAB algorithms, such as UCB, Thompson Sampling, and EXP3.Additive MAB algorithms have a wide range of applications, including in online advertising, recommendation systems, and clinical trials.

## Visit these resource for further learning

- [Contextual Bandits and Reinforcement Learning](https://towardsdatascience.com/contextual-bandits-and-reinforcement-learning-6bdfeaece72a)
- [Introduction to Multi-Armed Bandits](https://www.tensorflow.org/agents/tutorials/intro_bandit)
- [Multi-Armed Bandits](https://www.youtube.com/watch?v=BWPS-D5CDG0)
- [Bandit algorithm](https://www.youtube.com/watch?v=7F0jPUyb7m4)