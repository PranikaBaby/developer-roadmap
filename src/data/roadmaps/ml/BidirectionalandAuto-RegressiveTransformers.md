## Add BART (Bidirectional and Auto-Regressive Transformers) resource

## Introduction

BART (Bidirectional and Auto-Regressive Transformers) is a sequence-to-sequence model developed by Facebook AI Research (FAIR) that combines the strengths of bidirectional and auto-regressive models.BART was introduced in the paper "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension" by Mike Lewis et al. (2019). It has achieved state-of-the-art results on a range of natural language processing tasks, including machine translation, text summarization, and question answering.BART is pre-trained using a denoising autoencoder objective, where the model is trained to reconstruct the original sequence from a corrupted version. This pre-training enables BART to capture a wide range of syntactic and semantic features in text and can be fine-tuned for downstream tasks with relatively little data.

## Visit these resources for further information

- [BART Overview](https://huggingface.co/docs/transformers/model_doc/bart)
- [BART model in transformers](https://www.projectpro.io/recipes/what-is-bart-model-transformers)
- [Combining Bidirectional and Auto-Regressive Transformers](https://www.youtube.com/watch?v=1JBMCG8rW18)
- [Bart explained](https://www.youtube.com/watch?v=BGWpNQHIcs4&vl=en)